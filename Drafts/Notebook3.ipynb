{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/heidi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from neighbourhood import Neighbourhood\n",
    "from preprocessor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv(\"data/nodes.csv\")\n",
    "links = pd.read_csv(\"data/links_type.csv\")\n",
    "bilinks = pd.read_csv(\"data/bilinks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea of the Graph Neural Network\n",
    "### Graph Convolutional Network\n",
    "#### PinSAGE? Pins and Boards vs Items and Users vs Products and Customers\n",
    "\n",
    "1. Collecting **neighbourhood** of item and presenting the item through its neighbourhood. -> We need to define the neighbourhood of item in bipartite graph.\n",
    "2. Items **initial representation** should be embeddings created based on its features. -> We need to determine features for the items.\n",
    "3. ...\n",
    "4. PinSAGE creates embeddings based on item and its neighbourhood features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Collecting Neighbourhood in Bipartite graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neighbourhood for any Product item in our Bipartite graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = Neighbourhood(bilinks,links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random product and find its neighbourhood through common customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random product:  5465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([470733], [4.0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_product = np.random.choice(range(bilinks.Id.nunique()))\n",
    "print(\"Random product: \", random_product)\n",
    "N.find_neighbourhood(random_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Product's whole neighbourhood including direct neighbours and neighbourhood of common Customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([470733], [1.0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.get_neighbourhood(random_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Receptive field is neighbourhood of K randomly selected neighbours. Neighbours can be selected multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[470733]\n",
      "[470733, 470733]\n",
      "[470733, 470733, 470733]\n",
      "[470733, 470733, 470733, 470733]\n",
      "[470733, 470733, 470733, 470733, 470733]\n",
      "[470733, 470733, 470733, 470733, 470733, 470733]\n"
     ]
    }
   ],
   "source": [
    "for k in range(7):\n",
    "    print(N.get_receptive_field(random_product,K=k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random walk from Product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[147830, 142912, 143845, 142912, 147830]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.generate_random_walk(143845)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance pooling: selecting T (default 10) most important neighbours of Product by generating T random walks from T-hop neighbourhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{470733: 0.5, 5465: 0.5}\n"
     ]
    }
   ],
   "source": [
    "print(N.importance_pooling(random_product))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature table for Products (nodes)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to select different types of values: textual, categorical and numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\n",
    "    \"ID\": [\"Id\"],\n",
    "    \"textual\": [\"Title\"],\n",
    "    \"categorical\": [\"Group\"],\n",
    "    \"numerical\": [\"Salesrank\",\"AvgRating\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = get_features(nodes,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('int64'), dtype('float64'), dtype('int8')], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Features.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542664, 2004)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Features.shape # normaliseerimine? Standard scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional features from Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to add more information for nodes: categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat = pd.read_csv('data/categories.csv')\n",
    "# df_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each category path has its own unique CatId. We can represent each node as vectors representing which categories they belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat[\"nbr\"] = 1\n",
    "# Features2 = pd.pivot_table(data=df_cat,index=\"Id\",columns=\"CatId\",values=\"nbr\",fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat.Id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.min(df_cat.CatId.values),np.max(df_cat.CatId.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 542664-519781"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Graph neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture from Paper**\n",
    "\n",
    "Input: \n",
    "- Set of nodes $\\mathcal{M}\\subset \\mathcal{V}$ (minibatch from nodes $\\mathcal{V}$);\n",
    "- depth parameter $K$;\n",
    "- neighbourhood function $\\mathcal{N}:\\mathcal{V}\\rightarrow 2^{\\mathcal{V}}$\n",
    "\n",
    "Output:\n",
    "- Embeddings $z_u, \\forall u\\in \\mathcal{M}$\n",
    "\n",
    "Sampling neighbourhoods for nodes in minibatch:\n",
    "- $K$-th round consist of batchnodes: $\\mathcal{S}^{(k)} \\leftarrow \\mathcal{M}$;\n",
    "- for $k = K,\\dots, 1$ do\n",
    "  - $\\mathcal{S}^{(k-1)}\\leftarrow \\mathcal{S}^{(k)}$\n",
    "  - for $u\\in \\mathcal{S}^{(k)}$ do\n",
    "    - $\\mathcal{S}^{(k-1)}\\leftarrow\\mathcal{S}^{(k-1)}\\cup \\mathcal{N}(u)$ \n",
    "    - ($K-1$)-st round consist of $K$-th nodes and their neighbourhood nodes\n",
    "\n",
    "Generating embeddings for nodes in minibatch:\n",
    "- $h^{(0)}_u \\leftarrow x_u \\forall u\\in \\mathcal{S}^{(0)}$; init emb is feature vector $x_u$\n",
    "- for $k = 1,\\dots,K$ do\n",
    "  - for $u\\in \\mathcal{S}^{(k)}$ do\n",
    "    - $\\mathcal{H}\\leftarrow \\big\\{ h^{(k-1)}_v, \\forall v\\in \\mathcal{N}(u) \\big\\}$\n",
    "    - $h^{(k)}_u\\leftarrow \\text{convolve}^{(k)}\\big( h^{(k-1)}_u,\\mathcal{H} \\big)$\n",
    "- for $u\\in \\mathcal{M}$ do\n",
    "  - $z_u\\leftarrow G_2\\cdot\\text{ReLU}\\big( G_1h^{(K)}_u+g \\big)$\n",
    "\n",
    "<hr>\n",
    "<u>Convolve</u>:\n",
    "\n",
    "Input:\n",
    "- current embedding $z_u$ for node $u$;\n",
    "- set of neighbour embeddings $\\{ z_v|v\\in\\mathcal{N}(u) \\}$ with set of neighbour importances **$\\alpha$**;\n",
    "- symmetric vector function $\\gamma(\\cdot)$\n",
    "\n",
    "Output: \n",
    "- new embedding $z^{\\text{NEW}}_u$ for node u\n",
    "\n",
    "Generating an embedding:\n",
    "- neighbourhood embedding: $n_u\\leftarrow \\gamma\\big(\\{ \\text{ReLU}(Qh_v+q)|v\\in\\mathcal{N}(u) \\}, \\alpha \\big)$;\n",
    "- node $u$ embedding: $z^{\\text{NEW}}_u \\leftarrow \\text{ReLU}\\big(W\\cdot \\text{concat}(z_u,n_u)+w\\big)$;\n",
    "- normalized node $u$ emb: $z^{\\text{NEW}}_u \\leftarrow \\frac{z^{\\text{NEW}}_u}{\\lVert z^{\\text{NEW}}_u\\rVert _2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from input import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "pool_size = 7\n",
    "\n",
    "I = Input(Features, bilinks, links)\n",
    "NodeIDs = I.random_batch(batch_size)\n",
    "Nodes = I.init_embedding(NodeIDs)\n",
    "NeighbourIDs, Importances = I.pooling(NodeIDs,pool_size)\n",
    "Neighbourhoods, Alpha = I.init_neigh_embeddings(NeighbourIDs,Importances,pool_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Neighbouring =\n",
    "{\n",
    "    node1: {\n",
    "        \"neighbours\": [nodea, nodeb, nodec],\n",
    "        \"importances\": [0.55,0.25,0.2]},\n",
    "    node2: {\n",
    "        \"neighbours\": [nodex, nodey, nodez],\n",
    "        \"importances\": [0.45,0.45,0.1]},\n",
    "    nodea: {\n",
    "        \"neighbours\": ...,\n",
    "        \"importances\": ... },\n",
    "    nodeb: {\n",
    "        ... },\n",
    "    ...\n",
    "}\n",
    "Stacks = [\n",
    "[node1, node2],\n",
    "[node1, node2, nodea, nodeb, nodec, nodex, nodey, nodez],\n",
    "...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacks, Neighbouring = I.sampler(NodeIDs,pool_size=4,depth_K=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "5, 25, 229, \n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(Stacks))\n",
    "for S in Stacks: print(len(S),end=\", \")\n",
    "print(\"\")\n",
    "print(len(Neighbouring))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2004)\n",
      "(5, 7, 2004)\n",
      "(5, 7)\n"
     ]
    }
   ],
   "source": [
    "print(Nodes.shape)\n",
    "print(Neighbourhoods.shape)\n",
    "print(Alpha.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Layer, Dense, Concatenate, Lambda, Reshape, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator(Layer):\n",
    "    def __init__(self):\n",
    "        super(Aggregator, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Neighbours = inputs[0] # (7, 2004)\n",
    "        Alphas = inputs[1] # (1, 7)\n",
    "        Neighbourhood = Dense(20,activation='relu',dtype='float32')(Neighbours) # (7, 20)\n",
    "        return tf.matmul(Alphas,Neighbourhood) # (1, 7) x (7, 20) = (1, 20)\n",
    "\n",
    "class Convolve(Layer):\n",
    "    def __init__(self):\n",
    "        super(Convolve, self).__init__()\n",
    "\n",
    "    def call(self, inputs): # inputs = [Node, Neighbours,Alpha]\n",
    "        Node, Neighbours, Alpha = inputs\n",
    "        # Node = Input(shape=(1,2004)) \n",
    "        # Neighbours = Input(shape=(7,2004)) \n",
    "        # Alpha = Input(shape=(1,7))\n",
    "        NodeEmb = Dense(20,activation='relu',dtype='float32')(Node) # (1, 20)\n",
    "        print(NodeEmb.shape)\n",
    "        NeighboursEmb = Aggregator()([Neighbours,Alpha]) # (1, 20)\n",
    "        print(NeighboursEmb.shape)\n",
    "        AggEmb = Concatenate()([NodeEmb,NeighboursEmb]) # (1, 40)\n",
    "        print(AggEmb.shape)\n",
    "        Emb = Dense(20,activation='relu',dtype='float32')(AggEmb) # (1, 20)\n",
    "        return tf.divide(Emb,tf.norm(Emb,ord=2)) # (None, 1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 18:34:27.146156: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 20)\n",
      "(None, 1, 20)\n",
      "(None, 1, 40)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1, 20) dtype=float32 (created by layer 'tf.math.truediv')>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Convolve()([Nodes,Neighbourhoods,Alpha])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **NodeIDs** - list of Product Id-s -- the batch `(batch_size x 1)`\n",
    "- **NeighbourIDs** - list of important Neighbours' Ids -- pool of batch nodes `(batch_size x T)`\n",
    "- **Importances** - list of imp Neighbours' importances -- importances of each node in pool `(batch_size x T)`\n",
    "- **Hoods** - Features of important Neighbours -- features of pools `(batch_size x T x feature_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 3, 1, 1), dtype=tf.float32, name=None), name='tf.stack/stack:0', description=\"created by layer 'tf.stack'\")\n"
     ]
    }
   ],
   "source": [
    "minibatch = Input(shape=(1,1)) # minibatch: NodeIDs\n",
    "# Sampler:\n",
    "pool = 4\n",
    "K = 3\n",
    "Stack3 = Lambda(lambda x: [x+1,x+2,x+3])(minibatch)\n",
    "Stack3 = tf.stack([Stack3[0],Stack3[1],Stack3[2]],axis=1)\n",
    "print(Stack3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 5\n",
    "pool = 4\n",
    "minibatch = Input(shape=(1,1)) # minibatch: NodeIDs\n",
    "miba = tf.make_ndarray(minibatch)\n",
    "neighbours = tf.constant(I.pooling(miba,pool)[0])\n",
    "importances = tf.constant(I.pooling(miba,pool)[1])\n",
    "allnodes = tf.concat([minibatch,neighbours])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes with neighbours:  tf.Tensor(\n",
      "[[175826]\n",
      " [477413]\n",
      " [432050]\n",
      " [106924]\n",
      " [452232]], shape=(5, 1), dtype=int32)\n",
      "\n",
      "Neighbours:  tf.Tensor(\n",
      "[[     0      0      0      0]\n",
      " [154611 294332 420897 477413]\n",
      " [ 34879 260074  47197 148374]\n",
      " [ 58262 106924 478404      0]\n",
      " [302920 435646 384171 507845]], shape=(5, 4), dtype=int32)\n",
      "\n",
      "Importances:  tf.Tensor(\n",
      "[[0.     0.     0.     0.    ]\n",
      " [0.1875 0.1875 0.0625 0.0625]\n",
      " [0.3125 0.25   0.125  0.0625]\n",
      " [0.4375 0.3125 0.1875 0.0625]\n",
      " [0.3125 0.25   0.125  0.125 ]], shape=(5, 4), dtype=float32)\n",
      "\n",
      "Nodes with neighb:  tf.Tensor(\n",
      "[[175826      0      0      0      0]\n",
      " [477413 154611 294332 420897 477413]\n",
      " [432050  34879 260074  47197 148374]\n",
      " [106924  58262 106924 478404      0]\n",
      " [452232 302920 435646 384171 507845]], shape=(5, 5), dtype=int32)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 4), dtype=int32, numpy=\n",
       "array([[     0,      0,      0,      0],\n",
       "       [154611,  81002, 294332, 420897],\n",
       "       [ 91637, 522778, 260074,  73690],\n",
       "       [478404,  58262, 461256, 106924],\n",
       "       [452232, 302920, 435646, 379412],\n",
       "       [     0,      0,      0,      0],\n",
       "       [154611, 477413, 420897, 502937],\n",
       "       [ 43617, 350998,  47197, 359084],\n",
       "       [ 58262, 478404, 106924, 461256],\n",
       "       [231890, 435646,  12152,  98728],\n",
       "       [     0,      0,      0,      0],\n",
       "       [420897, 154611, 134651, 294332],\n",
       "       [451100, 260074,  34879,  47197],\n",
       "       [461256, 199891, 504962, 339592],\n",
       "       [275207, 384171, 302920, 452232],\n",
       "       [     0,      0,      0,      0],\n",
       "       [477413, 154611, 294332, 502937],\n",
       "       [502029,  80655, 260074, 451100],\n",
       "       [     0,      0,      0,      0],\n",
       "       [ 52224, 507845, 280603, 106114]], dtype=int32)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input\n",
    "mb = I.random_batch(5)\n",
    "inp = tf.constant(mb,shape=(mb.size,1),dtype=\"int32\")\n",
    "print(\"Nodes with neighbours: \", inp)\n",
    "print(\"\")\n",
    "\n",
    "# Neighbours of input nodes\n",
    "nei = tf.constant(I.pooling(mb,4)[0])\n",
    "print(\"Neighbours: \", nei)\n",
    "print(\"\")\n",
    "\n",
    "# Neighbours of input nodes\n",
    "alf = tf.constant(I.pooling(mb,4)[1])\n",
    "print(\"Importances: \", alf)\n",
    "print(\"\")\n",
    "\n",
    "# Concatenate input with neighbours\n",
    "inpnei = tf.concat([inp,nei],axis=-1)\n",
    "print(\"Nodes with neighb: \", inpnei)\n",
    "print(\"\")\n",
    "\n",
    "# Flattening\n",
    "ns = tf.unstack(nei, axis=1)\n",
    "n = tf.concat([ns[i] for i in range(len(ns))],axis=0)\n",
    "tf.constant(I.pooling(n.numpy(),4)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[182649 428845 252963  95884]\n",
      " [141329 450640 151096 533523]\n",
      " [ 93966 435885 371659  34342]\n",
      " [     0      0      0      0]\n",
      " [138403 509369 317317 277166]], shape=(5, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[0.0625 0.0625 0.0625 0.0625]\n",
      " [0.4375 0.125  0.125  0.125 ]\n",
      " [0.4375 0.25   0.1875 0.0625]\n",
      " [0.     0.     0.     0.    ]\n",
      " [0.1875 0.0625 0.0625 0.0625]], shape=(5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(nei)\n",
    "print(alf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4, 2004)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_emb = I.init_embedding(n.numpy())\n",
    "nei_emb = n_emb.reshape(5,4,2004)\n",
    "nei_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "alf_br = tf.broadcast_to(tf.expand_dims(alf,axis=-1),[5,4,2004])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_emb = tf.reduce_sum(nei_emb*alf_br,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[1,2,3],[4,5,6]])\n",
    "proto_tensor = tf.make_tensor_proto(a)  # convert `tensor a` to a proto tensor\n",
    "tf.make_ndarray(proto_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(node_ids,pool_T,depth_K):\n",
    "    stacks, alphas = [],[]\n",
    "    node_ids = tf.constant(node_ids,dtype=\"int32\")\n",
    "    for k in range(depth_K): # k = 0,...,K-1\n",
    "        neigh_ids = tf.constant(I.pooling(node_ids,pool_T)[0]) # get neighbours\n",
    "        alfas = tf.constant(I.pooling(node_ids,pool_T)[1]) # get importances\n",
    "        node_ids = tf.constant(node_ids,shape=(node_ids.numpy().size,1),dtype=\"int32\") # reshape nodes\n",
    "        stack = tf.concat([node_ids,neigh_ids],axis=-1) # stack nodes with their neighbours\n",
    "        stacks.append(stack)\n",
    "        alphas.append(alfas)\n",
    "        if k > 0:\n",
    "            stacks[k] = tf.concat([stack,stacks[k-1]],axis=0)\n",
    "            alphas[k] = tf.concat([alfas,alphas[k-1]],axis=0)\n",
    "        node_ids = tf.concat(tf.unstack(neigh_ids, axis=1),axis=0) # flatten neighbours for next round nodes\n",
    "    return stacks[-1], alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sampling(node_ids,pool_T,depth_K):\n",
    "#     stacks, neighbours, alphas = [],[],[]\n",
    "#     node_ids = tf.constant(node_ids,dtype=\"int32\")\n",
    "#     for k in range(depth_K):\n",
    "#         #node_ids = tf.constant(node_ids,shape=(1,node_ids.size),dtype=\"int32\") # (5, 1)\n",
    "#         neigh_ids = tf.constant(I.pooling(node_ids,pool_T)[0]) # (5, 4)\n",
    "#         alfas = tf.constant(I.pooling(node_ids,pool_T)[1]) # (5, 4)\n",
    "#         node_ids = tf.constant(node_ids,shape=(node_ids.numpy().size,1),dtype=\"int32\") # (5, 1)\n",
    "#         stack = tf.concat([node_ids,neigh_ids],axis=-1) # (5, 5)\n",
    "#         stacks.append(stack)\n",
    "#         neighbours.append(neigh_ids)\n",
    "#         alphas.append(alfas)\n",
    "#         node_ids = tf.concat(tf.unstack(neigh_ids, axis=1),axis=0)\n",
    "#     return stacks, neighbours, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 5)\n",
      "(5, 4)\n",
      "(25, 4)\n",
      "(105, 4)\n"
     ]
    }
   ],
   "source": [
    "batch = I.random_batch(5)\n",
    "stack, alphas = sampling(batch,4,3)\n",
    "print(stack.shape)\n",
    "for a in alphas:\n",
    "    print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (5, 5)\n",
      "1 (25, 5)\n",
      "2 (105, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([105, 5])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stack = stacks[0]\n",
    "# print(0, stack.shape)\n",
    "# for i in range(1,len(stacks)):\n",
    "#     stack = tf.concat([stack,stacks[i]],axis=0)\n",
    "#     print(i, stack.shape)\n",
    "# stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(25, 4)\n",
      "(105, 4)\n"
     ]
    }
   ],
   "source": [
    "# importances = []\n",
    "# importances.append(alphas[0])\n",
    "# print(importances[0].shape)\n",
    "# for i in range(1,len(alphas)):\n",
    "#     importances.append(tf.concat([importances[i-1],alphas[i]],axis=0))\n",
    "#     print(importances[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(stack, importances, pool_T, depth_K, init_dim):\n",
    "    stack_sizes = [0]+[importances[i].shape[0] for i in range(len(importances))]\n",
    "    # get initial stack\n",
    "    stack_np = tf.concat(tf.unstack(stack,axis=1),axis=0).numpy() # flatten and numpy\n",
    "    stack_emb = tf.reshape(tf.constant(I.init_embedding(stack_np)), [-1, pool_T+1, init_dim]) # init embeddings\n",
    "    # layers:\n",
    "    for k in range(depth_K-1,-1,-1): # k = K-1,...,0\n",
    "        # nodes, neighbours, alphas -> CONV -> new embeddings\n",
    "        node_emb = tf.expand_dims(tf.unstack(stack_emb,axis=1)[0],axis=1)\n",
    "        neigh_emb = tf.reshape(tf.concat(tf.unstack(stack_emb,axis=1)[1:],axis=0), [-1, pool_T, stack_emb.shape[-1]])\n",
    "        alphas = tf.expand_dims(importances[k],axis=1)\n",
    "        new_emb = Convolve()([node_emb, neigh_emb, alphas])\n",
    "        if k > 0: # k = K-1,...,1\n",
    "            # new embeddings -> COMBINE -> next stack\n",
    "            splits,combined = [],[]\n",
    "            for i in range(1,len(stack_sizes[:k+2])):\n",
    "                split = tf.concat(tf.unstack(new_emb,axis=0)[stack_sizes[i-1]:stack_sizes[i]],axis=0)\n",
    "                splits.append(tf.expand_dims(split,axis=1))\n",
    "                if i > 1: \n",
    "                    reshape_split = tf.reshape(split,[-1,pool_T,split.shape[-1]])\n",
    "                    combined.append(tf.concat([splits[i-2],reshape_split],axis=1))\n",
    "            stack_emb = tf.concat(combined,axis=0)\n",
    "    return new_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PinSAGE\n",
    "from graph import BipartiteGraph\n",
    "\n",
    "G = BipartiteGraph(Features,bilinks,links)\n",
    "mdl = PinSAGE(G, pool_T=4, depth_K=3, emb_dim=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BipartiteGraph' object has no attribute 'random_subgraph'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/Notebook3.ipynb Cell 59'\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> <a href='vscode-notebook-cell:/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/Notebook3.ipynb#ch0000071?line=0'>1</a>\u001B[0m batch \u001B[39m=\u001B[39m G\u001B[39m.\u001B[39;49mrandom_subgraph(\u001B[39m5\u001B[39m)\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/Notebook3.ipynb#ch0000071?line=1'>2</a>\u001B[0m mdl\u001B[39m.\u001B[39mcompile(optimizer \u001B[39m=\u001B[39m tf\u001B[39m.\u001B[39mkeras\u001B[39m.\u001B[39moptimizers\u001B[39m.\u001B[39mAdam(learning_rate\u001B[39m=\u001B[39m\u001B[39m1e-3\u001B[39m))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'BipartiteGraph' object has no attribute 'random_subgraph'"
     ]
    }
   ],
   "source": [
    "batch = G.random_subgraph(5)\n",
    "mdl.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 1, 20)\n",
      "(105, 1, 20)\n",
      "(105, 1, 40)\n",
      "(25, 1, 20)\n",
      "(25, 1, 20)\n",
      "(25, 1, 40)\n",
      "(5, 1, 20)\n",
      "(5, 1, 20)\n",
      "(5, 1, 40)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1, 20), dtype=float32, numpy=\n",
       "array([[[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , 0.08099678, 0.23498493,\n",
       "         0.24243158, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.32936975, 0.45133516, 0.42156368, 0.        , 0.        ,\n",
       "         0.20280978, 0.27794722, 0.38285437, 0.17848478, 0.03373609]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , 0.0805148 , 0.00652521,\n",
       "         0.09415338, 0.00813679, 0.        , 0.        , 0.        ,\n",
       "         0.10833282, 0.16822295, 0.13277219, 0.        , 0.        ,\n",
       "         0.        , 0.06208873, 0.08875901, 0.        , 0.08205435]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(stack, alphas, pool_T=4, depth_K=3, init_dim=2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def embedding(stack, nodes, neighbours, alphas,pool_T,depth_K, init_dim):\n",
    "#     # initial embeddings\n",
    "#     stack_np = tf.reshape(stack,[stack.numpy().size]).numpy()\n",
    "#     stack_emb = tf.reshape(tf.constant(I.init_embedding(stack_np)), [-1, pool_T+1, init_dim])\n",
    "#     node_emb = tf.unstack(stack_emb,axis=1)[0]\n",
    "#     node_emb = tf.expand_dims(node_emb,axis=1)\n",
    "#     neigh_emb = tf.reshape(tf.concat(tf.unstack(stack_emb,axis=1)[1:],axis=0), [-1, pool_T, init_dim])\n",
    "#     #importances = tf.broadcast_to(tf.expand_dims(alphas[-1],axis=-1),[alphas[-1].shape[0],pool_T,init_dim])\n",
    "#     importances = tf.expand_dims(alphas[-1],axis=1)\n",
    "#     new_emb = Convolve()([node_emb, neigh_emb, importances])\n",
    "    \n",
    "#     # using previous embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 1, 20)\n",
      "(80, 1, 20)\n",
      "(80, 1, 40)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(80, 1, 20), dtype=float32, numpy=\n",
       "array([[[0.        , 0.        , 0.        , ..., 0.01369335,\n",
       "         0.02207531, 0.07849218]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.00028086, 0.0083801 ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.01452408]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.02514934]],\n",
       "\n",
       "       [[0.        , 0.        , 0.00122165, ..., 0.        ,\n",
       "         0.00608738, 0.00664893]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.00747036,\n",
       "         0.02758896, 0.07731792]]], dtype=float32)>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding(stack, nodes, neighbours, alphas,4,3, 2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "722d0d88aa9a267f74f6ce4019e656054862c95b26904de541466056aedb2cbf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('BPM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}