{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/heidi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from neighbourhood import Neighbourhood\n",
    "from preprocessor import *\n",
    "from graph import BipartiteGraph\n",
    "from model import PinSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv(\"data/nodes.csv\")\n",
    "links = pd.read_csv(\"data/links_type.csv\")\n",
    "bilinks = pd.read_csv(\"data/bilinks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea of the Graph Neural Network\n",
    "### Graph Convolutional Network\n",
    "#### PinSAGE? Pins and Boards vs Items and Users vs Products and Customers\n",
    "\n",
    "1. Collecting **neighbourhood** of item and presenting the item through its neighbourhood. -> We need to define the neighbourhood of item in bipartite graph.\n",
    "2. Items **initial representation** should be embeddings created based on its features. -> We need to determine features for the items.\n",
    "3. ...\n",
    "4. PinSAGE creates embeddings based on item and its neighbourhood features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Collecting Neighbourhood in Bipartite graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neighbourhood for any Product item in our Bipartite graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = Neighbourhood(bilinks,links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random product and find its neighbourhood through common customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random product:  34044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_product = np.random.choice(range(bilinks.Id.nunique()))\n",
    "print(\"Random product: \", random_product)\n",
    "N.find_neighbourhood(random_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Product's whole neighbourhood including direct neighbours and neighbourhood of common Customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.get_neighbourhood(random_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Receptive field is neighbourhood of K randomly selected neighbours. Neighbours can be selected multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for k in range(7):\n",
    "    print(N.get_receptive_field(random_product,K=k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random walk from Product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[147830, 143845, 521846, 143845, 142912]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.generate_random_walk(143845)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance pooling: selecting T (default 10) most important neighbours of Product by generating T random walks from T-hop neighbourhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(N.importance_pooling(random_product,T=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature table for Products (nodes)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to select different types of values: textual, categorical and numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\n",
    "    \"ID\": [\"Id\"],\n",
    "    \"textual\": [\"Title\"],\n",
    "    \"categorical\": [\"Group\"],\n",
    "    \"numerical\": [\"Salesrank\",\"AvgRating\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = get_features(nodes,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('int64'), dtype('float64'), dtype('int8')], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Features.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542664, 2004)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Features.shape # normaliseerimine? Standard scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional features from Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to add more information for nodes: categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat = pd.read_csv('data/categories.csv')\n",
    "# df_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each category path has its own unique CatId. We can represent each node as vectors representing which categories they belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat[\"nbr\"] = 1\n",
    "# Features2 = pd.pivot_table(data=df_cat,index=\"Id\",columns=\"CatId\",values=\"nbr\",fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat.Id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.min(df_cat.CatId.values),np.max(df_cat.CatId.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 542664-519781"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Graph neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture from Paper**\n",
    "\n",
    "Input: \n",
    "- Set of nodes $\\mathcal{M}\\subset \\mathcal{V}$ (minibatch from nodes $\\mathcal{V}$);\n",
    "- depth parameter $K$;\n",
    "- neighbourhood function $\\mathcal{N}:\\mathcal{V}\\rightarrow 2^{\\mathcal{V}}$\n",
    "\n",
    "Output:\n",
    "- Embeddings $z_u, \\forall u\\in \\mathcal{M}$\n",
    "\n",
    "Sampling neighbourhoods for nodes in minibatch:\n",
    "- $K$-th round consist of batchnodes: $\\mathcal{S}^{(k)} \\leftarrow \\mathcal{M}$;\n",
    "- for $k = K,\\dots, 1$ do\n",
    "  - $\\mathcal{S}^{(k-1)}\\leftarrow \\mathcal{S}^{(k)}$\n",
    "  - for $u\\in \\mathcal{S}^{(k)}$ do\n",
    "    - $\\mathcal{S}^{(k-1)}\\leftarrow\\mathcal{S}^{(k-1)}\\cup \\mathcal{N}(u)$ \n",
    "    - ($K-1$)-st round consist of $K$-th nodes and their neighbourhood nodes\n",
    "\n",
    "Generating embeddings for nodes in minibatch:\n",
    "- $h^{(0)}_u \\leftarrow x_u \\forall u\\in \\mathcal{S}^{(0)}$; init emb is feature vector $x_u$\n",
    "- for $k = 1,\\dots,K$ do\n",
    "  - for $u\\in \\mathcal{S}^{(k)}$ do\n",
    "    - $\\mathcal{H}\\leftarrow \\big\\{ h^{(k-1)}_v, \\forall v\\in \\mathcal{N}(u) \\big\\}$\n",
    "    - $h^{(k)}_u\\leftarrow \\text{convolve}^{(k)}\\big( h^{(k-1)}_u,\\mathcal{H} \\big)$\n",
    "- for $u\\in \\mathcal{M}$ do\n",
    "  - $z_u\\leftarrow G_2\\cdot\\text{ReLU}\\big( G_1h^{(K)}_u+g \\big)$\n",
    "\n",
    "<hr>\n",
    "<u>Convolve</u>:\n",
    "\n",
    "Input:\n",
    "- current embedding $z_u$ for node $u$;\n",
    "- set of neighbour embeddings $\\{ z_v|v\\in\\mathcal{N}(u) \\}$ with set of neighbour importances **$\\alpha$**;\n",
    "- symmetric vector function $\\gamma(\\cdot)$\n",
    "\n",
    "Output: \n",
    "- new embedding $z^{\\text{NEW}}_u$ for node u\n",
    "\n",
    "Generating an embedding:\n",
    "- neighbourhood embedding: $n_u\\leftarrow \\gamma\\big(\\{ \\text{ReLU}(Qh_v+q)|v\\in\\mathcal{N}(u) \\}, \\alpha \\big)$;\n",
    "- node $u$ embedding: $z^{\\text{NEW}}_u \\leftarrow \\text{ReLU}\\big(W\\cdot \\text{concat}(z_u,n_u)+w\\big)$;\n",
    "- normalized node $u$ emb: $z^{\\text{NEW}}_u \\leftarrow \\frac{z^{\\text{NEW}}_u}{\\lVert z^{\\text{NEW}}_u\\rVert _2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from keras.layers import Input, Dot\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from model import PinSAGE, Sampler, Embedder\n",
    "from graph import BipartiteGraph\n",
    "from tqdm import tqdm\n",
    "\n",
    "G = BipartiteGraph(Features,bilinks,links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(inputs, depth_K, pool_T):\n",
    "    stacks, alphas = [],[]\n",
    "    node_ids = tf.constant(inputs,dtype=\"int32\")\n",
    "    for k in tqdm(range(depth_K),\n",
    "                  desc=f\"Sampling neighbourhoods and importances for nodes\"): # k = 0,...,K-1\n",
    "        neigh_ids = tf.constant(G.pooling(node_ids,pool_T)[0]) # get neighbours\n",
    "        alpha = tf.constant(G.pooling(node_ids,pool_T)[1]) # get importances\n",
    "        node_ids = tf.constant(node_ids,shape=(node_ids.numpy().size,1),dtype=\"int32\") # reshape nodes\n",
    "        stack = tf.concat([node_ids,neigh_ids],axis=-1) # stack nodes with their neighbours\n",
    "        stacks.append(stack)\n",
    "        alphas.append(alpha)\n",
    "        if k > 0:\n",
    "            stacks[k] = tf.concat([stack,stacks[k-1]],axis=0)\n",
    "            alphas[k] = tf.concat([alpha,alphas[k-1]],axis=0)\n",
    "        node_ids = tf.concat(tf.unstack(neigh_ids, axis=1),axis=0) # flatten neighbours for next round nodes\n",
    "    return stacks[-1], alphas[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_posneg_samples(G, node_ids, nbrOfPos, nbrOfNeg):\n",
    "    targets, labels = [], []\n",
    "    pos_samples, pos_weights = [], []\n",
    "    neg_samples, neg_weights = [], []\n",
    "    pos, posw = G.pooling(node_ids,pool_size=nbrOfPos)\n",
    "    for idx in range(len(node_ids)):\n",
    "        for i in range(len(pos[idx])):\n",
    "            targets.append(node_ids[idx])\n",
    "            pos_samples.append(pos[idx][i])\n",
    "            pos_weights.append(posw[idx][i])\n",
    "            labels.append(1)\n",
    "        for j in range(nbrOfNeg):\n",
    "            targets.append(node_ids[idx])\n",
    "            neg_samples.append(np.random.choice(G.Features.Id.values))\n",
    "            neg_weights.append(0.)\n",
    "            labels.append(0)\n",
    "    return np.array(targets), np.array(labels), \\\n",
    "        np.array(pos_samples+neg_samples), np.array(pos_weights + neg_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_over(stack, depth_K, alphas_tensor=False):\n",
    "    if alphas_tensor: pool_T = stack.shape[1]\n",
    "    else: pool_T = stack.shape[1]-1\n",
    "    s = [int(stack.shape[0]*(pool_T-1)/(pool_T**(depth_K)-1))]\n",
    "    for i in range(1,depth_K):\n",
    "        s.append(s[i-1]*pool_T)\n",
    "    s = [0]+s\n",
    "    cums = np.cumsum(s)\n",
    "    pieces = []\n",
    "    for k in range(depth_K):\n",
    "        pieces.append(tf.unstack(stack,axis=0)[cums[k]:cums[k+1]])\n",
    "    for k in range(depth_K-1,0,-1):\n",
    "        pieces[k-1] = tf.concat(\n",
    "            [pieces[k-1]]\n",
    "            +[tf.unstack(pieces[k],axis=0)[int(i*s[k+1]/pool_T):int((i+1)*s[k+1]/pool_T)] for i in range(pool_T)],axis=1)\n",
    "    return pieces[k-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_back(stack, pool_T, depth_K, alphas_tensor=False):\n",
    "    if alphas_tensor: y = pool_T\n",
    "    else: y = pool_T + 1\n",
    "    pieces = [tf.concat(\n",
    "        [tf.reshape(tf.unstack(stack,axis=1)[i],shape=(stack.shape[0],1)) for i in range((y)*j, (y)*(j+1))]\n",
    "        ,axis=1) for j in range(int(stack.shape[1]/(y)))]\n",
    "    s = [pieces[0]]\n",
    "    for i in range((pool_T+1)):\n",
    "        for j in range(depth_K+1):\n",
    "            s.append(pieces[1+i+(pool_T+1)*j])\n",
    "    return tf.concat(s,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-16 14:01:22.696462: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Finding neighbours and importances: 100%|██████████| 5/5 [00:01<00:00,  3.83it/s] ?it/s]\n",
      "Finding neighbours and importances: 100%|██████████| 5/5 [00:00<00:00,  6.15it/s]\n",
      "Finding neighbours and importances: 100%|██████████| 20/20 [00:03<00:00,  5.97it/s]4,  2.16s/it]\n",
      "Finding neighbours and importances: 100%|██████████| 20/20 [00:03<00:00,  6.55it/s]\n",
      "Finding neighbours and importances: 100%|██████████| 80/80 [00:11<00:00,  6.85it/s]4,  4.66s/it]\n",
      "Finding neighbours and importances: 100%|██████████| 80/80 [00:11<00:00,  6.93it/s]\n",
      "Sampling neighbourhoods and importances for nodes: 100%|██████████| 3/3 [00:31<00:00, 10.61s/it]\n"
     ]
    }
   ],
   "source": [
    "batch = G.random_subgraph(5)\n",
    "stacks, alphas = sampling(batch,depth_K=3, pool_T=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([105, 5]), TensorShape([105, 4]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacks.shape, alphas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([40, 52]), TensorShape([40, 39]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_over(stacks,depth_K=3).shape, stack_over(alphas,depth_K=3,alphas_tensor=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([105, 5]), TensorShape([105, 4]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_back(stack_over(stacks,depth_K=3),pool_T=4,depth_K=3).shape,stack_back(stack_over(alphas,depth_K=3,alphas_tensor=True),pool_T=4,depth_K=3, alphas_tensor=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1, 20), dtype=float32, numpy=\n",
       "array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.47997394e-01, 0.00000000e+00, 5.77476807e-02, 3.78233850e-01,\n",
       "         0.00000000e+00, 6.02768511e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "         2.13850021e-01, 0.00000000e+00, 1.13240167e-01, 5.71299940e-02]],\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.59668431e-01, 6.80349185e-05,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.01896435e-01,\n",
       "         5.99330179e-02, 0.00000000e+00, 1.57962404e-02, 0.00000000e+00,\n",
       "         1.73881471e-01, 1.28060440e-02, 8.67689103e-02, 0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.26779988e-01, 0.00000000e+00, 0.00000000e+00, 5.05898893e-02,\n",
       "         9.57183540e-02, 0.00000000e+00, 8.88336450e-02, 4.55566317e-01,\n",
       "         0.00000000e+00, 0.00000000e+00, 4.24623042e-02, 0.00000000e+00,\n",
       "         2.88389206e-01, 0.00000000e+00, 1.65953457e-01, 5.04132174e-03]],\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.02190204e-01, 0.00000000e+00, 5.83346561e-02, 3.06845456e-01,\n",
       "         0.00000000e+00, 4.63655591e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.97280005e-01, 0.00000000e+00, 7.55265206e-02, 2.03500520e-02]],\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.99985144e-02,\n",
       "         8.19928646e-02, 0.00000000e+00, 4.72850017e-02, 3.23525071e-01,\n",
       "         0.00000000e+00, 5.16727678e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.96409911e-01, 0.00000000e+00, 0.00000000e+00, 1.42399266e-01]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Embedder(G, 3, 4, 20)([stack_over(stacks,3),stack_over(alphas,3,alphas_tensor=True)])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding neighbours and importances: 100%|██████████| 10/10 [00:00<00:00, 18.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40, 40, 40, 40)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = G.random_subgraph(10)\n",
    "targets, labels, samples, weights = generate_posneg_samples(G, batch, 2, 2)\n",
    "targets.size, labels.size, samples.size, weights.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea:\n",
    "- Generate positive and negative samples for nodes with labels 1 and 0\n",
    "- \"target\", \"context\", \"weight\", \"label\"\n",
    "- write the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding neighbours and importances: 100%|██████████| 40/40 [00:02<00:00, 16.40it/s]it/s]\n",
      "Finding neighbours and importances: 100%|██████████| 40/40 [00:02<00:00, 16.89it/s]\n",
      "Finding neighbours and importances: 100%|██████████| 120/120 [00:08<00:00, 14.55it/s]  4.82s/it]\n",
      "Finding neighbours and importances: 100%|██████████| 120/120 [00:07<00:00, 15.37it/s]\n",
      "Finding neighbours and importances: 100%|██████████| 360/360 [00:25<00:00, 13.99it/s] 11.44s/it]\n",
      "Finding neighbours and importances: 100%|██████████| 360/360 [00:25<00:00, 14.30it/s]\n",
      "Sampling neighbourhoods and importances for nodes: 100%|██████████| 3/3 [01:11<00:00, 23.94s/it]\n",
      "Finding neighbours and importances: 100%|██████████| 40/40 [00:03<00:00, 10.76it/s]it/s]\n",
      "Finding neighbours and importances: 100%|██████████| 40/40 [00:11<00:00,  3.63it/s]\n",
      "Finding neighbours and importances: 100%|██████████| 120/120 [00:13<00:00,  9.09it/s] 14.77s/it]\n",
      "Finding neighbours and importances: 100%|██████████| 120/120 [00:09<00:00, 12.41it/s]\n",
      "Finding neighbours and importances: 100%|██████████| 360/360 [00:32<00:00, 11.06it/s] 19.64s/it]\n",
      "Finding neighbours and importances: 100%|██████████| 360/360 [00:32<00:00, 11.01it/s]\n",
      "Sampling neighbourhoods and importances for nodes: 100%|██████████| 3/3 [01:43<00:00, 34.36s/it]\n"
     ]
    }
   ],
   "source": [
    "stacks, alphas = sampling(targets,3,3)\n",
    "stacks_s, alphas_s = sampling(samples,3,3)\n",
    "targets, samples = [stacks, alphas], [stacks_s, alphas_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([520, 4]), TensorShape([520, 3]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacks_s.shape, alphas_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [stack_over(stacks,depth_K=3),stack_over(alphas,depth_K=3,alphas_tensor=True)]\n",
    "samples = [stack_over(stacks_s,depth_K=3),stack_over(alphas_s,depth_K=3,alphas_tensor=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 52)\n",
      "(40, 39)\n",
      "(40, 52)\n",
      "(40, 39)\n"
     ]
    }
   ],
   "source": [
    "for set in targets+samples: print(set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(targets, samples, labels, weights, batch_size):\n",
    "    inputs = {\n",
    "        \"stack_t\": targets[0],\n",
    "        \"alpha_t\": targets[1],\n",
    "        \"stack_s\": samples[0],\n",
    "        \"alpha_s\": samples[1]\n",
    "    }\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels, weights))\n",
    "    dataset = dataset.shuffle(buffer_size=batch_size * 2)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 10\n",
    "dataset = create_dataset(\n",
    "    targets=targets,\n",
    "    samples=samples,\n",
    "    labels=labels,\n",
    "    weights=weights,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siiamaani töötab... saaks nüüd mudeli ka tööle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "StagingError",
     "evalue": "Exception encountered when calling layer \"embedder_2\" (type Embedder).\n\nin user code:\n\n    File \"/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/model.py\", line 64, in call  *\n        stack = stack_back(inputs[0],self.pool_T,self.depth_K)\n    File \"/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/model.py\", line 95, in stack_back  *\n        pieces = [tf.concat(\n\n    IndexError: list index out of range\n\n\nCall arguments received:\n  • inputs=['tf.Tensor(shape=(None,), dtype=int32)', 'tf.Tensor(shape=(None,), dtype=int32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStagingError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/Notebook4.ipynb Cell 52'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/Notebook4.ipynb#ch0000053?line=0'>1</a>\u001b[0m inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/Notebook4.ipynb#ch0000053?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstack_t\u001b[39m\u001b[39m\"\u001b[39m: Input(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstack_t\u001b[39m\u001b[39m\"\u001b[39m, shape\u001b[39m=\u001b[39m(), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mint32\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/Notebook4.ipynb#ch0000053?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39malpha_t\u001b[39m\u001b[39m\"\u001b[39m: Input(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39malpha_t\u001b[39m\u001b[39m\"\u001b[39m, shape\u001b[39m=\u001b[39m(), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mint32\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/Notebook4.ipynb#ch0000053?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstack_s\u001b[39m\u001b[39m\"\u001b[39m: Input(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstack_s\u001b[39m\u001b[39m\"\u001b[39m, shape\u001b[39m=\u001b[39m(), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mint32\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/Notebook4.ipynb#ch0000053?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39malpha_s\u001b[39m\u001b[39m\"\u001b[39m: Input(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39malpha_s\u001b[39m\u001b[39m\"\u001b[39m, shape\u001b[39m=\u001b[39m(), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mint32\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/Notebook4.ipynb#ch0000053?line=5'>6</a>\u001b[0m }\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/Notebook4.ipynb#ch0000053?line=6'>7</a>\u001b[0m target_emb \u001b[39m=\u001b[39m Embedder(G, \u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m20\u001b[39;49m)([inputs[\u001b[39m\"\u001b[39;49m\u001b[39mstack_t\u001b[39;49m\u001b[39m\"\u001b[39;49m],inputs[\u001b[39m\"\u001b[39;49m\u001b[39malpha_t\u001b[39;49m\u001b[39m\"\u001b[39;49m]])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/Notebook4.ipynb#ch0000053?line=7'>8</a>\u001b[0m sample_emb \u001b[39m=\u001b[39m Embedder(G, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m20\u001b[39m)([inputs[\u001b[39m\"\u001b[39m\u001b[39mstack_s\u001b[39m\u001b[39m\"\u001b[39m],inputs[\u001b[39m\"\u001b[39m\u001b[39malpha_s\u001b[39m\u001b[39m\"\u001b[39m]])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/Notebook4.ipynb#ch0000053?line=9'>10</a>\u001b[0m logits \u001b[39m=\u001b[39m Dot(axes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, normalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msimilarity\u001b[39m\u001b[39m\"\u001b[39m)([target_emb, sample_emb])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NS/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/NS/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/NS/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///opt/anaconda3/envs/NS/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/NS/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/NS/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NS/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/NS/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py?line=689'>690</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/NS/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py?line=690'>691</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/NS/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py?line=691'>692</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/NS/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py?line=692'>693</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/NS/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py?line=693'>694</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mStagingError\u001b[0m: Exception encountered when calling layer \"embedder_2\" (type Embedder).\n\nin user code:\n\n    File \"/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/model.py\", line 64, in call  *\n        stack = stack_back(inputs[0],self.pool_T,self.depth_K)\n    File \"/Users/heidi/Documents/Andmeteadus/NetworkScience/Project/network_science/model.py\", line 95, in stack_back  *\n        pieces = [tf.concat(\n\n    IndexError: list index out of range\n\n\nCall arguments received:\n  • inputs=['tf.Tensor(shape=(None,), dtype=int32)', 'tf.Tensor(shape=(None,), dtype=int32)']"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"stack_t\": Input(name=\"stack_t\", shape=(), dtype=\"int32\"),\n",
    "    \"alpha_t\": Input(name=\"alpha_t\", shape=(), dtype=\"int32\"),\n",
    "    \"stack_s\": Input(name=\"stack_s\", shape=(), dtype=\"int32\"),\n",
    "    \"alpha_s\": Input(name=\"alpha_s\", shape=(), dtype=\"int32\")\n",
    "}\n",
    "target_emb = Embedder(G, 3, 3, 20)([inputs[\"stack_t\"],inputs[\"alpha_t\"]])\n",
    "sample_emb = Embedder(G, 3, 3, 20)([inputs[\"stack_s\"],inputs[\"alpha_s\"]])\n",
    "\n",
    "logits = Dot(axes=1, normalize=False, name=\"similarity\")([target_emb, sample_emb])\n",
    "\n",
    "pinsage = Model(inputs=inputs, outputs=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bdc8687d8d6dbe19d9fb3ab8a3c1b4cf8d9b0b1677a9be6df582fe49a179ef2b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
